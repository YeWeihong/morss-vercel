# Morss 全文抓取原理详解

## 目录
1. [概述](#概述)
2. [全文抓取的工作流程](#全文抓取的工作流程)
3. [核心算法：readabilite 内容提取](#核心算法readabilite-内容提取)
4. [Full-text 和 Original 模式的区别](#full-text-和-original-模式的区别)
5. [为什么有些文章能抓取全文，有些不能](#为什么有些文章能抓取全文有些不能)
6. [如何提高全文抓取成功率](#如何提高全文抓取成功率)
7. [技术细节与源码分析](#技术细节与源码分析)

---

## 概述

Morss 是一个用于获取完整全文 RSS 订阅源的工具。它的核心功能是：

1. **下载 RSS 订阅源**：获取包含文章标题、摘要和链接的 RSS feed
2. **访问文章页面**：根据 RSS 中的链接，访问每篇文章的原始网页
3. **提取正文内容**：使用智能算法从 HTML 页面中提取文章主体内容
4. **重新组装 RSS**：将提取的完整内容放回 RSS 订阅源中

这个过程的关键在于**第三步**——如何从复杂的网页 HTML 中准确识别并提取文章正文。

---

## 全文抓取的工作流程

### 1. 整体流程图

```
用户请求
   ↓
FeedFetch (获取 RSS 订阅源)
   ↓
FeedGather (填充文章内容)
   ├─→ ItemFix (修复链接)
   ├─→ ItemFill (下载并提取全文) ← 核心步骤
   └─→ ItemAfter (后处理)
   ↓
FeedFormat (格式化输出)
   ↓
返回完整订阅源
```

### 2. FeedFetch 阶段（获取订阅源）

**位置**：`morss/morss.py` 中的 `FeedFetch()` 函数

**功能**：
- 使用 `crawler.adv_get()` 下载 RSS 订阅源
- 支持多种订阅源格式：RSS 2.0、Atom、RDF
- 解析 XML/HTML 并提取文章列表
- 支持缓存机制，避免重复下载

**代码示例**：
```python
def FeedFetch(url, options):
    req = crawler.adv_get(url=url, timeout=TIMEOUT)
    rss = feeds.parse(req['data'], url=url, encoding=req['encoding'])
    return req['url'], rss
```

### 3. FeedGather 阶段（填充内容）

**位置**：`morss/morss.py` 中的 `FeedGather()` 函数

**功能**：
- 遍历订阅源中的每个文章条目
- 对每个条目执行 `ItemFill()` 函数
- 控制并发和时间限制（防止过载）

**关键参数**（环境变量）：
- `MAX_ITEM`：最多抓取多少篇文章的全文（默认 5 篇）
- `MAX_TIME`：最多花多少秒抓取全文（默认 2 秒）
- `LIM_ITEM`：最多保留多少篇文章（默认 10 篇）
- `LIM_TIME`：整个处理过程的时间限制（默认 2.5 秒）

### 4. ItemFill 阶段（核心：下载并提取全文）

**位置**：`morss/morss.py` 中的 `ItemFill()` 函数

**功能**：
```python
def ItemFill(item, options, feedurl='/', fast=False):
    # 1. 检查是否有链接
    if not item.link:
        return True
    
    # 2. 下载文章页面
    req = crawler.adv_get(url=item.link, timeout=TIMEOUT)
    
    # 3. 检查是否为 HTML 页面
    if req['contenttype'] not in crawler.MIMETYPE['html']:
        return True
    
    # 4. 使用 readabilite 提取正文
    out = readabilite.get_article(
        req['data'], 
        url=req['url'], 
        encoding_in=req['encoding'],
        xpath=options.xpath  # 可选的自定义 XPath 规则
    )
    
    # 5. 如果提取成功，替换 item.content
    if out is not None:
        item.content = out
    
    return True
```

**关键点**：
- 如果 `options.proxy` 为 `True`，则**不会执行** `ItemFill()`，保留原始内容
- 如果提取失败（`out is None`），则**保留原始内容**不变
- 支持自定义 XPath 规则（`--xpath` 参数）来手动指定内容提取规则

---

## 核心算法：readabilite 内容提取

### 1. 算法概述

**位置**：`morss/readabilite.py` 中的 `get_article()` 函数

Morss 使用自己实现的 **readabilite** 算法（受 Mozilla Readability 启发），这是一个基于**启发式评分系统**的内容提取算法。

### 2. 工作原理

#### 步骤 1：解析 HTML

```python
html = parse(data, encoding_in)
```

使用 `lxml` 和 `BeautifulSoup` 解析 HTML 文档，构建 DOM 树。

#### 步骤 2：节点评分

**核心函数**：`score_node(node)`

为 HTML 文档中的每个节点（标签）计算一个分数，分数越高，越可能是文章正文。

**评分规则**：

1. **标签类型加分/减分**：
   ```python
   # 好标签（加分）
   tags_good = ['h1', 'h2', 'h3', 'article', 'p', 'cite', 'section']
   # 坏标签（减分）
   tags_bad = ['a', 'aside', 'footer', 'form', 'input']
   # 危险标签（直接移除）
   tags_dangerous = ['script', 'head', 'iframe', 'style']
   ```

2. **CSS 类名和 ID 加分/减分**：
   ```python
   # 好的类名（加分）
   class_good = ['article', 'body', 'content', 'entry', 'main', 'post']
   # 坏的类名（减分）
   class_bad = ['comment', 'sidebar', 'ad-', 'menu', 'footer', 'social']
   ```

3. **文本内容长度加分**：
   ```python
   # 每 10 个单词加 1 分，最多加 3 分
   score += min(int(word_count / 10), 3)
   ```

4. **链接密度惩罚**：
   ```python
   # 如果链接文本占总文本的比例过高，则降低分数
   link_ratio = link_word_count / total_word_count
   score = score * (1 - 2 * link_ratio)
   ```

**示例**：
```html
<div class="article-content" id="main">  <!-- 加分：class 和 id 包含好词 -->
    <p>这是一段正文...</p>               <!-- 加分：p 标签 + 文本内容 -->
    <p>另一段正文...</p>
</div>
<div class="sidebar ad-banner">          <!-- 减分：class 包含坏词 -->
    <a href="#">广告链接</a>              <!-- 减分：链接密度高 -->
</div>
```

#### 步骤 3：分数传播

**核心函数**：`spread_score(node, score)`

将节点的分数传播给其父节点、祖父节点，使得包含多个高分节点的容器获得更高分数。

```python
# 分数逐级递减传播
def spread_score(node, score):
    delta = score / 2
    for ancestor in [node,] + list(node.iterancestors()):
        if score >= 1:
            ancestor.score += score
            score -= delta
```

#### 步骤 4：选择最佳节点

**核心函数**：`get_best_node(html, threshold=5)`

```python
# 1. 对所有节点评分
score_all(html)

# 2. 按分数从高到低排序
ranked_nodes = sorted(html.iter(), key=lambda x: get_score(x), reverse=True)

# 3. 检查最高分是否达到阈值
if get_score(ranked_nodes[0]) < threshold:
    return None  # 没有找到合适的内容

# 4. 找到前两个高分节点的最近公共祖先
if len(ranked_nodes) > 1:
    best = lowest_common_ancestor(ranked_nodes[0], ranked_nodes[1], 3)
else:
    best = ranked_nodes[0]

return best
```

#### 步骤 5：清理节点

**核心函数**：`clean_root(best, keep_threshold)`

从最佳节点开始，递归清理其子节点：

1. **移除危险标签**：`script`、`style`、`iframe` 等
2. **移除垃圾标签**：`form`、`input`、`footer` 等
3. **移除低分节点**：分数低于阈值的节点
4. **移除高链接密度节点**：链接文本占比超过 80% 的节点
5. **清理属性**：只保留 `title`、`src`、`href`、`type`、`value` 等必要属性

```python
def clean_node(node, keep_threshold):
    # 移除评论
    if isinstance(node, lxml.html.HtmlComment):
        parent.remove(node)
    
    # 移除危险标签
    if node.tag in tags_dangerous:
        parent.remove(node)
    
    # 保留高分节点
    if get_score(node) >= keep_threshold:
        return
    
    # 移除垃圾标签
    if node.tag in tags_junk:
        parent.remove(node)
    
    # 移除高链接密度节点
    if link_word_count / word_count > 0.8:
        parent.remove(node)
```

#### 步骤 6：最终验证

检查提取的内容是否合格：

```python
# 统计文本内容
wc = count_words(best.text_content())
wca = count_words(' '.join([x.text_content() for x in best.findall('.//a')]))

# 如果文本太少或链接密度太高，则认为提取失败
if wc - wca < 50 or float(wca) / wc > 0.3:
    return None  # 提取失败
```

**质量标准**：
- 非链接文本至少 50 个单词
- 链接文本占比不超过 30%

#### 步骤 7：修复链接并返回

```python
# 将相对链接转换为绝对链接
if url:
    best.make_links_absolute(url)

# 返回 HTML 字符串
return lxml.etree.tostring(best, method='html', encoding=encoding_out)
```

### 3. 算法流程图

```
HTML 文档
   ↓
解析为 DOM 树
   ↓
遍历所有节点并评分
   ├─→ 标签类型评分
   ├─→ 类名/ID 评分
   ├─→ 文本长度评分
   └─→ 链接密度惩罚
   ↓
分数传播到父节点
   ↓
选择最高分节点
   ↓
找到最近公共祖先
   ↓
清理子节点
   ├─→ 移除危险标签
   ├─→ 移除垃圾标签
   ├─→ 移除低分节点
   └─→ 清理属性
   ↓
质量验证
   ├─→ 检查文本长度
   └─→ 检查链接密度
   ↓
修复链接
   ↓
返回提取的内容
```

### 4. 评分示例

假设有以下 HTML：

```html
<html>
<body>
  <div class="header">网站标题</div>
  <div class="sidebar ad">
    <a href="#">广告1</a>
    <a href="#">广告2</a>
  </div>
  <div class="main-content" id="article">
    <h1>文章标题</h1>
    <p>这是第一段正文，包含很多有用的信息...</p>
    <p>这是第二段正文，继续讲述故事...</p>
    <p>这是第三段正文，得出结论。</p>
  </div>
  <div class="footer">版权信息</div>
</body>
</html>
```

**评分结果**：

| 节点 | 标签分 | 类名分 | 文本分 | 链接惩罚 | 最终分 |
|------|--------|--------|--------|----------|---------|
| `<div class="header">` | 0 | 0 | 0.5 | 0 | **0.5** |
| `<div class="sidebar ad">` | 0 | -2 | 0.5 | -80% | **-1.6** |
| `<div class="main-content">` | 0 | +3 | 0 | 0 | **3** |
| `<h1>` (在 main-content 内) | +4 | 0 | 0.5 | 0 | **4.5** |
| `<p>` × 3 (在 main-content 内) | +12 | 0 | 3 | 0 | **15** |

**最终选择**：`<div class="main-content">` 及其子节点（总分最高）

---

## Full-text 和 Original 模式的区别

### 1. Full-text 模式（默认模式）

**启用方式**：不使用 `--proxy` 参数

**行为**：
1. Morss 会访问每篇文章的链接
2. 下载完整的 HTML 页面
3. 使用 readabilite 算法提取正文
4. 用提取的内容**替换**原始 RSS 中的简短摘要

**优点**：
- ✅ 获得完整的文章内容
- ✅ 可以离线阅读完整文章
- ✅ 不需要点击链接就能看全文

**缺点**：
- ❌ 需要更多时间处理（每篇文章都要下载和解析）
- ❌ 可能提取失败（如果网页结构复杂）
- ❌ 增加服务器负载

**适用场景**：
- 你想在 RSS 阅读器中直接阅读完整文章
- 目标网站的 RSS 只提供摘要
- 你有稳定的网络和足够的处理时间

**示例**：

```bash
# Full-text 模式
morss https://example.com/feed.xml
# 或
http://localhost:8000/https://example.com/feed.xml
```

**输出**：
```xml
<item>
  <title>文章标题</title>
  <link>https://example.com/article/123</link>
  <description>
    <h1>文章标题</h1>
    <p>这是第一段完整的正文内容...</p>
    <p>这是第二段完整的正文内容...</p>
    <p>这是第三段完整的正文内容...</p>
    <!-- 完整的文章内容 -->
  </description>
</item>
```

### 2. Original 模式（--proxy 模式）

**启用方式**：使用 `--proxy` 参数

**行为**：
1. Morss 只下载 RSS 订阅源
2. **不访问**文章链接
3. **不提取**全文内容
4. 保留原始 RSS 中的内容（标题、摘要、链接）

**优点**：
- ✅ 处理速度快（不需要下载文章页面）
- ✅ 减少服务器负载
- ✅ 避免被目标网站屏蔽

**缺点**：
- ❌ 只能看到摘要，无法获得全文
- ❌ 需要点击链接才能阅读完整文章

**适用场景**：
- 你只想快速浏览标题和摘要
- 目标网站的 RSS 已经包含足够的信息
- 你想减少处理时间和服务器负载
- 用于测试或调试 RSS 订阅源

**示例**：

```bash
# Original 模式
morss --proxy https://example.com/feed.xml
# 或
http://localhost:8000/:proxy/https://example.com/feed.xml
```

**输出**：
```xml
<item>
  <title>文章标题</title>
  <link>https://example.com/article/123</link>
  <description>这是文章的简短摘要...</description>
  <!-- 保留原始摘要，不下载全文 -->
</item>
```

### 3. 代码实现

**位置**：`morss/morss.py` 中的 `FeedGather()` 函数

```python
def FeedGather(rss, url, options):
    for i, item in enumerate(sorted_items):
        item = ItemFix(item, options, url)
        
        # 关键判断：如果 options.proxy 为 True，则跳过 ItemFill
        if not options.proxy:
            ItemFill(item, options, url)  # 下载并提取全文
        
        item = ItemAfter(item, options)
    
    return rss
```

### 4. 对比表格

| 特性 | Full-text 模式 | Original 模式 (--proxy) |
|------|----------------|------------------------|
| **访问文章链接** | ✅ 是 | ❌ 否 |
| **提取全文** | ✅ 是 | ❌ 否 |
| **处理时间** | 🐌 较慢（需下载和解析） | ⚡ 快速（只下载 RSS） |
| **服务器负载** | 🔥 较高 | 💧 较低 |
| **输出内容** | 📄 完整文章 | 📋 原始摘要 |
| **适用场景** | 想离线阅读全文 | 只想快速浏览标题 |
| **失败风险** | 可能提取失败 | 几乎不会失败 |

### 5. 混合使用建议

你可以结合使用其他参数来优化体验：

#### 5.1 使用 `--clip` 参数

在 Full-text 模式下，使用 `--clip` 参数可以**同时保留原始摘要和全文**：

```bash
morss --clip https://example.com/feed.xml
```

**输出**：
```xml
<description>
  这是原始摘要...
  <br/><br/><hr/><br/><br/>
  <h1>文章标题</h1>
  <p>这是完整正文...</p>
</description>
```

#### 5.2 使用 `--cache` 参数

只使用缓存的内容，不下载新文章：

```bash
morss --cache https://example.com/feed.xml
```

这相当于一个"半 proxy"模式：
- 如果文章已经在缓存中，使用缓存的全文
- 如果文章不在缓存中，不下载（类似 --proxy）

#### 5.3 控制处理数量

通过环境变量控制全文提取的数量：

```bash
# 只提取前 3 篇文章的全文，其余保留原始摘要
MAX_ITEM=3 morss https://example.com/feed.xml
```

---

## 为什么有些文章能抓取全文，有些不能

### 1. 成功提取全文的条件

要成功提取全文，需要满足以下所有条件：

#### 条件 1：网页结构相对标准

**成功的例子**：
```html
<article class="post-content">
  <h1>文章标题</h1>
  <p>第一段内容...</p>
  <p>第二段内容...</p>
</article>
```

- ✅ 使用语义化标签（`<article>`、`<section>`、`<main>`）
- ✅ 正文包含在清晰的容器中
- ✅ 类名包含 "content"、"article"、"post" 等关键词

**失败的例子**：
```html
<div id="x1">
  <div class="a">文章</div>
  <div class="a">标题</div>
  <div class="a">第一段...</div>
  <div class="a">第二段...</div>
</div>
```

- ❌ 没有语义化标签
- ❌ 类名无意义
- ❌ 结构扁平，难以识别

#### 条件 2：正文内容足够长

```python
# readabilite.py 中的验证逻辑
wc = count_words(best.text_content())
wca = count_words(links.text_content())

if wc - wca < 50:  # 非链接文本少于 50 个单词
    return None  # 提取失败
```

**成功的例子**：
- ✅ 文章包含 200+ 单词的正文
- ✅ 段落清晰，结构完整

**失败的例子**：
- ❌ 文章只有 1-2 句话
- ❌ 内容主要是图片/视频，文字很少
- ❌ 内容是列表/表格，缺少段落文本

#### 条件 3：链接密度不高

```python
if float(wca) / wc > 0.3:  # 链接文本占比超过 30%
    return None  # 提取失败
```

**成功的例子**：
```html
<div class="content">
  <p>这是一段很长的正文内容，包含丰富的信息和细节...</p>
  <p>另一段正文...</p>
  <p>参考：<a href="#">某链接</a></p>
</div>
```
- ✅ 链接文本占比低于 30%

**失败的例子**：
```html
<div class="content">
  <p><a href="#">链接1</a></p>
  <p><a href="#">链接2</a></p>
  <p><a href="#">链接3</a></p>
  <p>只有一小段正文。</p>
</div>
```
- ❌ 链接文本占比过高（被认为是导航菜单或链接列表）

#### 条件 4：页面不是特殊类型

```python
if req['contenttype'] not in crawler.MIMETYPE['html']:
    return True  # 跳过，不提取
```

**成功的例子**：
- ✅ 内容类型是 `text/html`
- ✅ 标准的 HTML 网页

**失败的例子**：
- ❌ PDF 文件（`application/pdf`）
- ❌ 图片（`image/jpeg`）
- ❌ 视频（`video/mp4`）
- ❌ JSON 数据（`application/json`）

#### 条件 5：评分达到阈值

```python
if get_score(ranked_nodes[0]) < threshold:  # 默认阈值 = 5
    return None  # 没有找到合适的内容
```

**成功的例子**：
```html
<div class="main-content article-body">
  <h1>文章标题</h1>           <!-- +4 分（h1 标签） -->
  <p>第一段正文...</p>        <!-- +4 分（p 标签）+ 文本长度 -->
  <p>第二段正文...</p>        <!-- +4 分（p 标签）+ 文本长度 -->
</div>
<!-- 总分 > 5，提取成功 -->
```

**失败的例子**：
```html
<div id="main">
  <span>文章标题</span>       <!-- 0 分（span 不加分） -->
  <div>第一段正文...</div>    <!-- 0 分（div 不加分） -->
</div>
<!-- 总分 < 5，提取失败 -->
```

### 2. 常见失败场景

#### 场景 1：JavaScript 渲染的网站（SPA）

**问题**：
- 网站使用 React、Vue、Angular 等前端框架
- 内容通过 JavaScript 动态加载
- Morss 只能看到初始 HTML（通常是空的）

**示例**：
```html
<!-- Morss 下载到的 HTML -->
<div id="root"></div>
<script src="app.js"></script>
<!-- 没有任何文章内容 -->
```

**解决方案**：
- ❌ 无法直接解决（Morss 不执行 JavaScript）
- ✅ 使用该网站的 RSS feed（如果有）
- ✅ 寻找该网站的移动版或简化版（通常使用服务端渲染）

#### 场景 2：反爬虫措施

**问题**：
- 网站检测到 bot 访问，返回 403/验证码页面
- 需要登录才能查看完整内容
- 使用 Cloudflare 或其他 DDoS 防护

**示例**：
```html
<!-- Morss 下载到的 HTML -->
<h1>Access Denied</h1>
<p>You have been blocked...</p>
```

**解决方案**：
- ✅ 配置随机 User-Agent（Morss 默认已支持）
- ✅ 使用代理服务器（配置 `HTTP_PROXY` 环境变量）
- ✅ 降低抓取频率（调整 `MAX_ITEM` 和 `TIMEOUT`）
- ❌ 对于需要登录的内容，无法自动处理

#### 场景 3：复杂的页面布局

**问题**：
- 正文内容分散在多个 `<div>` 中
- 广告、侧边栏、评论与正文混在一起
- 使用了表格或复杂的 CSS 布局

**示例**：
```html
<div class="container">
  <div class="ad">广告</div>
  <div class="content-part-1">第一部分正文</div>
  <div class="ad">广告</div>
  <div class="content-part-2">第二部分正文</div>
  <div class="sidebar">侧边栏</div>
</div>
```

**解决方案**：
- ✅ 使用 `--xpath` 参数手动指定内容位置
  ```bash
  morss --xpath '//div[contains(@class,"content-part")]' https://example.com/feed.xml
  ```
- ✅ 为该网站创建自定义规则（在 `feedify.ini` 中）

#### 场景 4：内容主要是图片/视频

**问题**：
- 文章是图集或视频，文字描述很少
- readabilite 依赖文本内容评分，会认为这不是有效文章

**示例**：
```html
<div class="gallery">
  <img src="photo1.jpg" />
  <img src="photo2.jpg" />
  <img src="photo3.jpg" />
  <p>简短的图片说明。</p>
</div>
```

**解决方案**：
- ✅ 使用 `--xpath` 手动指定包含图片的容器
- ❌ 纯视频内容无法提取（需要原网站提供 RSS）

#### 场景 5：付费墙或限制访问

**问题**：
- 文章需要订阅或付费才能查看
- 网站只显示前几段，后面被隐藏

**示例**：
```html
<div class="article">
  <p>前几段可见内容...</p>
  <div class="paywall">
    <p>订阅后继续阅读</p>
  </div>
</div>
```

**解决方案**：
- ❌ Morss 无法绕过付费墙（这是合法限制）
- ✅ 如果你有账号，可以尝试配置 Cookie（需要修改代码）

#### 场景 6：网页编码问题

**问题**：
- 网页使用特殊编码（如 GB2312、Big5）
- Morss 未正确检测编码，导致乱码

**解决方案**：
- ✅ Morss 使用 `chardet` 自动检测编码（通常能解决）
- ✅ 如果仍有问题，可以在代码中手动指定编码

### 3. 成功率估计

基于实际使用经验：

| 网站类型 | 成功率 | 说明 |
|---------|--------|------|
| **传统新闻网站** | 80-90% | 结构标准，适合提取 |
| **博客平台** | 70-85% | 大多数博客结构清晰 |
| **社交媒体** | 20-40% | 结构复杂，需自定义规则 |
| **现代 SPA 网站** | 10-30% | 依赖 JavaScript，难以提取 |
| **论坛** | 40-60% | 取决于具体实现 |
| **电商网站** | 50-70% | 产品描述通常可提取 |

---

## 如何提高全文抓取成功率

### 1. 使用自定义 XPath 规则

当 readabilite 自动提取失败时，可以手动指定内容位置。

#### 方法 1：使用 --xpath 参数

```bash
morss --xpath '//div[@class="article-content"]' https://example.com/feed.xml
```

#### 方法 2：在 URL 中指定（Web 服务）

```
http://localhost:8000/:xpath=|div[@class="article-content"]/https://example.com/feed.xml
```

**注意**：URL 中 `/` 需要替换为 `|`（参见 `XPATH_CUSTOM_FEEDS_CN.md`）

#### 如何找到正确的 XPath：

1. 在浏览器中打开文章页面
2. 右键点击文章正文 → "检查元素"
3. 在开发者工具中找到包含正文的标签
4. 右键点击该标签 → "Copy" → "Copy XPath"
5. 简化 XPath（移除不必要的索引）

**示例**：

浏览器给出的 XPath：
```
/html/body/div[2]/div[1]/div[3]/article/div[1]
```

简化后的 XPath：
```
//article/div[@class="content"]
```

### 2. 创建自定义 Feed 规则

对于经常访问的网站，可以在 `morss/feedify.ini` 中创建规则。

**位置**：`morss/feedify.ini`

**格式**：
```ini
[example.com]
path = https://example.com/*
mode = html
items = //article
item_link = .//a[@class="title"]/@href
item_title = .//h2[@class="title"]
item_content = .//div[@class="content"]
item_time = .//time/@datetime
```

**示例**：为知乎专栏创建规则
```ini
[zhihu.com]
path = https://zhuanlan.zhihu.com/*
mode = html
items = //div[@class="Post-RichTextContainer"]
item_link = //link[@rel="canonical"]/@href
item_title = //h1[@class="Post-Title"]
item_content = //div[@class="RichText"]
item_time = //div[@class="ContentItem-time"]/text()
```

### 3. 优化环境变量

根据你的需求调整参数：

```bash
# 增加单篇文章的处理时间
TIMEOUT=10

# 增加全文提取的数量
MAX_ITEM=20

# 增加总处理时间
MAX_TIME=30
LIM_TIME=60

# 运行 Morss
docker run -p 8000:8000 \
  -e TIMEOUT=10 \
  -e MAX_ITEM=20 \
  -e MAX_TIME=30 \
  morss
```

**注意**：增加这些值会提高成功率，但也会增加处理时间和服务器负载。

### 4. 使用缓存提高速度

```bash
# 使用磁盘缓存
docker run -p 8000:8000 \
  -e CACHE=diskcache \
  -e CACHE_SIZE=10737418240 \
  -v /path/to/cache:/cache \
  morss
```

**好处**：
- 已下载的文章不需要重新访问
- 即使网站临时不可访问，仍能从缓存获取内容
- 减少目标网站的负载

### 5. 配置代理（针对反爬虫）

```bash
# 设置 HTTP 代理
export HTTP_PROXY=http://proxy.example.com:8080
export HTTPS_PROXY=http://proxy.example.com:8080

# 运行 Morss
morss https://example.com/feed.xml
```

**或者**在 Docker 中：
```bash
docker run -p 8000:8000 \
  -e HTTP_PROXY=http://proxy.example.com:8080 \
  -e HTTPS_PROXY=http://proxy.example.com:8080 \
  morss
```

### 6. 调整阈值（高级）

修改 `morss/morss.py` 中的 `ItemFill()` 函数：

```python
# 原始代码
out = readabilite.get_article(req['data'], url=req['url'], 
                                encoding_in=req['encoding'], 
                                encoding_out='unicode', 
                                threshold=5)  # 默认阈值

# 降低阈值（更宽松，但可能提取到无关内容）
out = readabilite.get_article(req['data'], url=req['url'], 
                                encoding_in=req['encoding'], 
                                encoding_out='unicode', 
                                threshold=3)
```

**注意**：降低阈值可能导致提取到广告、侧边栏等无关内容。

### 7. 贡献规则到社区

如果你为某个网站创建了有效的 XPath 规则，可以：

1. Fork Morss 项目
2. 将规则添加到 `morss/feedify.ini`
3. 提交 Pull Request

这样其他用户也能受益。

---

## 技术细节与源码分析

### 1. 核心文件结构

```
morss/
├── morss.py           # 主逻辑：FeedFetch, FeedGather, FeedFormat
│   ├── ItemFill()     # 核心函数：下载并提取全文
│   ├── FeedGather()   # 遍历 RSS 项目，决定是否提取全文
│   └── process()      # 统一入口函数
│
├── readabilite.py     # 内容提取算法
│   ├── get_article()       # 主函数：提取文章正文
│   ├── score_node()        # 为节点评分
│   ├── spread_score()      # 分数传播
│   ├── get_best_node()     # 选择最佳节点
│   └── clean_root()        # 清理无关内容
│
├── crawler.py         # HTTP 请求处理
│   ├── adv_get()           # 高级 HTTP 请求（带缓存、重定向等）
│   ├── custom_opener()     # 自定义 HTTP opener
│   └── 各种 Handler        # Cookie、重定向、缓存等处理
│
├── feeds.py           # RSS 解析和生成
│   ├── parse()             # 解析 RSS/Atom/HTML/JSON
│   ├── FeedXML             # RSS/Atom 解析器
│   ├── FeedHTML            # HTML 解析器（自定义规则）
│   └── FeedJSON            # JSON 解析器
│
└── caching.py         # 缓存系统
    ├── DiskCacheHandler    # 磁盘缓存
    ├── RedisCacheHandler   # Redis 缓存
    └── MemCacheHandler     # 内存缓存
```

### 2. 关键代码片段

#### 2.1 决定是否提取全文

**位置**：`morss/morss.py`

```python
def FeedGather(rss, url, options):
    for i, item in enumerate(sorted_items):
        # 软限制：超过 MAX_ITEM 或 MAX_TIME 后，只使用缓存
        if time.time() - start_time > max_time >= 0 or i + 1 > max_item >= 0:
            if not options.proxy:
                if ItemFill(item, options, url, fast=True) is False:
                    item.remove()  # 缓存中也没有，删除该项
                    continue
        
        # 硬限制内：正常提取全文
        else:
            if not options.proxy:
                ItemFill(item, options, url)  # fast=False，会下载新内容
```

**逻辑**：
- 前 `MAX_ITEM` 篇文章：下载并提取全文
- 之后的文章：只使用缓存，不下载新内容
- 超过 `LIM_ITEM` 的文章：直接删除

#### 2.2 内容提取核心逻辑

**位置**：`morss/readabilite.py`

```python
def get_article(data, url=None, encoding_in=None, encoding_out='unicode', 
                debug=False, threshold=5, xpath=None):
    # 1. 解析 HTML
    html = parse(data, encoding_in)
    
    # 2. 如果提供了 XPath，直接使用
    if xpath is not None:
        xpath_match = html.xpath(xpath)
        if len(xpath_match):
            best = xpath_match[0]
        else:
            best = get_best_node(html, threshold)  # XPath 失败，回退到自动提取
    else:
        best = get_best_node(html, threshold)  # 自动提取
    
    # 3. 未找到合适内容
    if best is None:
        return None
    
    # 4. 清理节点
    if not debug:
        keep_threshold = get_score(best) * 3/4
        clean_root(best, keep_threshold)
    
    # 5. 质量验证
    wc = count_words(best.text_content())
    wca = count_words(' '.join([x.text_content() for x in best.findall('.//a')]))
    
    if not debug and (wc - wca < 50 or float(wca) / wc > 0.3):
        return None  # 质量不合格
    
    # 6. 修复链接并返回
    if url:
        best.make_links_absolute(url)
    
    return lxml.etree.tostring(best, method='html', encoding=encoding_out)
```

#### 2.3 节点评分系统

**位置**：`morss/readabilite.py`

```python
def score_node(node):
    score = 0
    class_id = (node.get('class') or '') + (node.get('id') or '')
    
    # 标签类型评分
    if node.tag in tags_junk:
        score += -2
    elif node.tag in tags_bad:
        score += -1
    elif node.tag in tags_good:
        score += 4
    
    # 类名/ID 评分
    if regex_bad.search(class_id):
        score += -1
    if regex_good.search(class_id):
        score += 3
    
    # 文本长度评分
    wc = count_words(node.text_content())
    score += min(int(wc / 10), 3)  # 每 10 个单词加 1 分，最多 3 分
    
    # 链接密度惩罚
    if wc != 0:
        wca = count_words(' '.join([x.text_content() for x in node.findall('.//a')]))
        score = score * (1 - 2 * float(wca) / wc)
    
    return score
```

### 3. 缓存机制

Morss 使用三层缓存：

#### 3.1 HTTP 缓存

基于 `ETag` 和 `Last-Modified` 头：

```python
# crawler.py
if 'etag' in headers:
    cache_data['etag'] = headers['etag']

if 'last-modified' in headers:
    cache_data['last-modified'] = headers['last-modified']

# 下次请求时发送 If-None-Match 和 If-Modified-Since
if 'etag' in cache_data:
    req.add_header('If-None-Match', cache_data['etag'])

if 'last-modified' in cache_data:
    req.add_header('If-Modified-Since', cache_data['last-modified'])
```

**好处**：
- 如果内容未更改，服务器返回 304，不传输内容
- 节省带宽和时间

#### 3.2 内容缓存

缓存已提取的文章内容：

```python
# morss.py
def ItemFill(item, options, feedurl='/', fast=False):
    # 如果 fast=True，强制使用缓存
    if fast or options.cache:
        policy = 'offline'  # 只使用缓存，不下载
    elif options.force:
        policy = 'refresh'  # 忽略缓存，强制下载
    else:
        policy = None  # 自动决定
    
    req = crawler.adv_get(url=item.link, policy=policy, ...)
```

#### 3.3 持久化缓存

支持三种后端：

```python
# 内存缓存（默认）
cache = {}

# 磁盘缓存
import diskcache
cache = diskcache.Cache('/path/to/cache')

# Redis 缓存
import redis
cache = redis.Redis(host='localhost', port=6379)
```

**配置**：
```bash
# 磁盘缓存
CACHE=diskcache DISKCACHE_DIR=/cache morss https://example.com/feed.xml

# Redis 缓存
CACHE=redis REDIS_HOST=localhost REDIS_PORT=6379 morss https://example.com/feed.xml
```

### 4. User-Agent 轮换

为避免被屏蔽，Morss 随机选择 User-Agent：

```python
# crawler.py
DEFAULT_UAS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 ...",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_4) AppleWebKit/605.1.15 ...",
    # ... 共 10 个不同的 UA
]

def custom_opener(...):
    ua = random.choice(DEFAULT_UAS)
    req.add_header('User-Agent', ua)
```

### 5. 性能优化

#### 5.1 并发控制

通过时间和数量限制避免过载：

```python
MAX_ITEM = 5      # 最多抓取 5 篇
MAX_TIME = 2      # 最多花 2 秒
LIM_ITEM = 10     # 最多保留 10 篇
LIM_TIME = 2.5    # 总时间不超过 2.5 秒
```

#### 5.2 懒加载

只有在需要时才下载文章内容，而不是一次性下载所有：

```python
for i, item in enumerate(sorted_items):
    if i + 1 > max_item:
        # 超过限制，跳过
        break
    
    ItemFill(item, options, url)
```

#### 5.3 缓存预热

首次访问后，后续访问速度大幅提升：

```bash
# 第一次：下载并缓存
morss https://example.com/feed.xml  # 耗时 5 秒

# 第二次：使用缓存
morss https://example.com/feed.xml  # 耗时 0.5 秒
```

---

## 总结

### 关键点回顾

1. **Full-text 模式**：
   - 下载文章页面
   - 使用 readabilite 算法提取正文
   - 替换原始摘要

2. **Original 模式（--proxy）**：
   - 只下载 RSS 订阅源
   - 不访问文章链接
   - 保留原始摘要

3. **成功提取全文的条件**：
   - 网页结构标准（语义化标签、清晰的类名）
   - 正文内容足够长（非链接文本 ≥ 50 个单词）
   - 链接密度不高（≤ 30%）
   - 评分达到阈值（≥ 5 分）
   - 不是 JavaScript 渲染的 SPA
   - 没有反爬虫措施阻止访问

4. **提高成功率的方法**：
   - 使用 `--xpath` 参数手动指定内容位置
   - 在 `feedify.ini` 中创建自定义规则
   - 优化环境变量（`MAX_ITEM`、`MAX_TIME`、`TIMEOUT`）
   - 使用缓存提高速度
   - 配置代理绕过反爬虫

5. **核心算法**：
   - 基于启发式评分的内容提取
   - 标签类型、类名、文本长度、链接密度综合评分
   - 分数传播到父节点
   - 选择最高分节点作为文章正文

---

## 相关文档

- **[项目流程说明.md](项目流程说明.md)** - 项目整体架构和数据流转
- **[XPATH_CUSTOM_FEEDS_CN.md](XPATH_CUSTOM_FEEDS_CN.md)** - 自定义 XPath 规则详解
- **[CRAWLER_ANTI_SCRAPING_CN.md](CRAWLER_ANTI_SCRAPING_CN.md)** - 爬虫机制与反爬策略
- **[README.md](README.md)** - 项目介绍和使用指南

---

## 常见问题

### Q1：为什么我的文章只提取到了标题？

**A**：可能原因：
- 网页使用 JavaScript 动态加载内容
- readabilite 评分太低，未找到合适的内容
- 文章被付费墙阻止

**解决方案**：
- 使用 `--xpath` 手动指定内容位置
- 检查网页源代码，确认内容是否在 HTML 中

### Q2：如何知道 Morss 是否成功提取了全文？

**A**：对比输出：
```bash
# 原始 RSS（使用 --proxy）
morss --proxy https://example.com/feed.xml

# 全文 RSS（默认模式）
morss https://example.com/feed.xml

# 对比 <description> 或 <content> 标签的长度
```

### Q3：可以同时提取多个 RSS 订阅源吗？

**A**：不能在一个命令中同时处理多个 URL，但可以：
```bash
# 方法 1：使用循环
for feed in feed1.xml feed2.xml feed3.xml; do
    morss https://example.com/$feed
done

# 方法 2：使用后台任务
morss https://example.com/feed1.xml &
morss https://example.com/feed2.xml &
morss https://example.com/feed3.xml &
wait
```

### Q4：Morss 会保存我的浏览历史吗？

**A**：取决于缓存配置：
- **内存缓存**（默认）：重启后丢失
- **磁盘缓存**：保存在指定目录
- **Redis 缓存**：保存在 Redis 服务器

缓存内容包括：
- 下载的 RSS 订阅源
- 下载的文章 HTML
- 提取的文章正文
- HTTP 头（ETag、Last-Modified）

### Q5：如何调试提取失败的问题？

**A**：
```bash
# 1. 开启调试模式
DEBUG=1 morss https://example.com/feed.xml

# 2. 单独测试 readabilite
python -m morss.readabilite https://example.com/article/123

# 3. 使用浏览器开发者工具
# - 右键 → 检查元素
# - 找到包含正文的标签
# - 复制 XPath
# - 使用 --xpath 参数测试
```

---

**最后更新**：2026-01-17  
**作者**：基于 Morss 源码分析生成  
**许可证**：GNU AGPLv3
